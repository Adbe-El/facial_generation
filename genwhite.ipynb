{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helper   #this file is in the folder...helper.py\n",
    "# import dataset\n",
    "data_dir='/images'\n",
    "\n",
    "# Image configuration\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "data_files = glob(os.path.join(data_dir, 'white/*.jpg'))\n",
    "shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, 3\n",
    "\n",
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if image.size != (width, height):\n",
    "        # Remove most pixels that aren't part of a face\n",
    "        face_width = face_height = 108\n",
    "        j = (image.size[0] - face_width) // 2\n",
    "        i = (image.size[1] - face_height) // 2\n",
    "        image = image.crop([j, i, j + face_width, i + face_height])\n",
    "        image = image.resize([width, height], Image.BILINEAR)\n",
    "\n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "def get_batch(image_files, width, height, mode='RGB'):\n",
    "    \"\"\"\n",
    "    Get a single image\n",
    "    \"\"\"\n",
    "    data_batch = np.array(\n",
    "        [get_image(sample_file, width, height, mode) for sample_file in image_files]).astype(np.float32)\n",
    "\n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "\n",
    "    return data_batch\n",
    "\n",
    "def get_batches(batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches\n",
    "    \"\"\"\n",
    "    IMAGE_MAX_VALUE = 255\n",
    "\n",
    "\n",
    "    current_index = 0\n",
    "    while current_index + batch_size <= shape[0]:\n",
    "        data_batch = get_batch(\n",
    "            data_files[current_index:current_index + batch_size],\n",
    "            *shape[1:3])\n",
    "\n",
    "        current_index += batch_size\n",
    "\n",
    "        yield data_batch / IMAGE_MAX_VALUE - 0.5\n",
    "\n",
    "\n",
    "\n",
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_real = tf.placeholder(tf.float32, shape=(None, image_width, image_height, image_channels), name='input_real') \n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return inputs_real, inputs_z, learning_rate\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(images, reuse=False):\n",
    "    \"\"\"\n",
    "    Create the discriminator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # using 4 layer network as in DCGAN Paper\n",
    "        \n",
    "        # Conv 1\n",
    "        conv1 = tf.layers.conv2d(images, 64, 5, 2, 'SAME')\n",
    "        lrelu1 = tf.maximum(alpha * conv1, conv1)\n",
    "        \n",
    "        # Conv 2\n",
    "        conv2 = tf.layers.conv2d(lrelu1, 128, 5, 2, 'SAME')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training=True)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Conv 3\n",
    "        conv3 = tf.layers.conv2d(lrelu2, 256, 5, 1, 'SAME')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training=True)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "       \n",
    "        # Flatten\n",
    "        flat = tf.reshape(lrelu3, (-1, 4*4*256))\n",
    "        \n",
    "        # Logits\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        \n",
    "        # Output\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits\n",
    "\n",
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse=False if is_train==True else True):\n",
    "        # First fully connected layer\n",
    "        x_1 = tf.layers.dense(z, 2*2*512)\n",
    "        \n",
    "        # Reshape it to start the convolutional stack\n",
    "        deconv_2 = tf.reshape(x_1, (-1, 2, 2, 512))\n",
    "        batch_norm2 = tf.layers.batch_normalization(deconv_2, training=is_train)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Deconv 1\n",
    "        deconv3 = tf.layers.conv2d_transpose(lrelu2, 256, 5, 2, padding='VALID')\n",
    "        batch_norm3 = tf.layers.batch_normalization(deconv3, training=is_train)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "        \n",
    "        \n",
    "        # Deconv 2\n",
    "        deconv4 = tf.layers.conv2d_transpose(lrelu3, 128, 5, 2, padding='SAME')\n",
    "        batch_norm4 = tf.layers.batch_normalization(deconv4, training=is_train)\n",
    "        lrelu4 = tf.maximum(alpha * batch_norm4, batch_norm4)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.conv2d_transpose(lrelu4, out_channel_dim, 5, 2, padding='SAME')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    \"\"\"\n",
    "    \n",
    "    label_smoothing = 0.9\n",
    "    \n",
    "    g_model = generator(input_z, out_channel_dim)\n",
    "    d_model_real, d_logits_real = discriminator(input_real)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                labels=tf.ones_like(d_model_real) * label_smoothing))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "                                                  \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.ones_like(d_model_fake) * label_smoothing))\n",
    "    \n",
    "    \n",
    "    return d_loss, g_loss\n",
    "\n",
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    \"\"\"\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt\n",
    "\n",
    "def show_generator_output(sess, n_images, input_z, out_channel_dim):\n",
    "    \"\"\"\n",
    "    Show example output for the generator\n",
    "    \"\"\"\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "    pyplot.imshow(helper.images_square_grid(samples))\n",
    "    pyplot.show()\n",
    "    \n",
    "\n",
    "def train(epoch_count, batch_size, z_dim, learning_rate, beta1, data_shape):\n",
    "    \"\"\"\n",
    "    Train the GAN\n",
    "    \"\"\"\n",
    "    input_real, input_z, _ = model_inputs(data_shape[1], data_shape[2], data_shape[3], z_dim)\n",
    "    d_loss, g_loss = model_loss(input_real, input_z, data_shape[3])\n",
    "    d_opt, g_opt = model_opt(d_loss, g_loss, learning_rate, beta1)\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for batch_images in get_batches(batch_size):\n",
    "                \n",
    "                # values range from -0.5 to 0.5, therefore scale to range -1, 1\n",
    "                batch_images = batch_images * 2\n",
    "                steps += 1\n",
    "            \n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "                \n",
    "                _ = sess.run(d_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "                _ = sess.run(g_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "                \n",
    "                if steps % 400 == 0:\n",
    "                    # At the end of every 10 epochs, get the losses and print them out\n",
    "                    train_loss_d = d_loss.eval({input_z: batch_z, input_real: batch_images})\n",
    "                    train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "\n",
    "                    print(\"Epoch {}/{}...\".format(epoch_i+1, epoch_count),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                    \n",
    "        _ = show_generator_output(sess, 1, input_z, data_shape[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAG+9JREFUeJztnXt0lOd17p8taSShC0ISIO4IEBdjg7ko2A6+EWMHEzd2ktax13KO3aYm7UmyTnratEnak7qXtE7bNMlq3LQkduLktEmc40vs2I5vcYLtxGDAIGHu5i6EBAiQ0H2kff5gyJId3mcwiBl5vc9vLS1J82jre+fTPPpmZr97b3N3CCHiIyfbCxBCZAeZX4hIkfmFiBSZX4hIkfmFiBSZX4hIkfmFiBSZX4hIkfmFiJS8TB6sqKjIy8pGBPV+8N2GlkvEJP8/Zuinuuca1UF2Qvb18diE8fuVYzw+mUyzCzMvHO9I0lCzfKrnWJrz1svPe1+iM6gVpDl2b0+ax0MOP3a/h9eeB37O+/q5npPga+tP83gz8mBO83BBkiyt7fhxdLZ3pHkwn+K8zG9mywB8HUAugG+7+73s58vKRuD371oR1Dutix4vMTys9beU0thCP0n1zvJCqqOzJyidOMH+KwFjC8OxAFCaz499pJnH91UkwhpaaGxBYhLViwr4ees6VEL142Pqg1pN7ngae/AAv9+FReQBAaCzty2oVVoBjT3Ryf+mRWP7qH4S7VRP5IXXXsBtgCP54X96D/3HSh48gHN+2m+n/nXdB+BGALMB3G5ms8/19wkhMsv5vOZfBGCnu+9y9x4APwRw8+AsSwhxoTkf848HsH/A9wdSt70FM1thZmvNbG1HR8d5HE4IMZhc8Hf73X2lu9e6e21RUdGFPpwQ4iw5H/M3AJg44PsJqduEEO8Czsf8rwGYbmZT7FS+6DYAjw/OsoQQF5pzTvW5e9LMPgXgGZxK9T3g7m+wGEM/ErnhvG9Hbhk9ZmlPd1jL56mVhrIKquckD1O9qyOcEitua6axvSN4Ku9kYxXVp43ia9teFM4pF42fRWMXtrVSfaNNoPr+xnCaEQCWlM8PaseOH6GxOX1zqF4+oZfqpZ3lQW1vK3+SOnXYUar3NvH3r9qn89TzJftGB7U9Ffy8VB8Np/EL+LaOt3BeeX53fwrAU+fzO4QQ2UHbe4WIFJlfiEiR+YWIFJlfiEiR+YWIFJlfiEjJaD0/LAeeGy6l7DvBSzj788PlqS1HxvBjJ/g+gM6uqVQfNWZ3UGtIs0cgv3Mm1Zvz91M9p28Y1dsawwXgU5p5aeqeIn7sxuQoqtf0NVF9a2t4D0J9B8/TL7lyNdXHPs4L31+dF+4dMamE761oHMVLdot3hPerAECRjaV6e97moDbtJD/ne8eH97sk0/QZGIiu/EJEiswvRKTI/EJEiswvRKTI/EJEiswvRKRkNtWX48CwcAplwkne7vhIbzh9snscT83MHcHTJ+OeW0P1EzXh0tZhk3ia0I6Fu8gCQE1vJdXbcjZS/fDI24La8d5Haeyl3fwhsPs9v6L6ye2XUL2reF1QW7IhTQfcksVU313D04wjisJp5YlHx9FYf/01qvdM/BDVK/Ey1U9eMjeodb2xjcYWHHlPULMkTwsPRFd+ISJF5hciUmR+ISJF5hciUmR+ISJF5hciUmR+ISIlo3l+78tB34lwG+vOxHEaf7ib5G1bw22aAWBKH582u/um7VQvWB9uIz135y4a29/G21vvnVpH9SO7aqhe3R8uy+2ZvYzGbjzwANVnrOFTeN+8hE+Drq4Pt0zfMZr/7vIneKl0wad52eyM4+GS3vWj99LYvr18X0hRLo8feYCvfV9jeO2je3nb77acLUGtD2lG/A5AV34hIkXmFyJSZH4hIkXmFyJSZH4hIkXmFyJSZH4hIuW88vxmtgdAG4A+AEl3r2U/73lJdI88FtRzesJjiwFgZHe4/vuiXB67/8hzVC88+EWq57V+L6htn30jjV09LdxqGQBu+9YSqi+84XmqrxkXrvdfdZTn+ZeXfZnqk/JWUf2Zn/NeBpvfH967cc0IXjNfdoSvfd39O6i+afGUoLa0kLfubp/FW3PvODab68v4CO9pXw4f/9gEfr8mjA7vA8h/B9fzwdjks8Td+UBxIcSQQ0/7hYiU8zW/A3jWzNaZ2YrBWJAQIjOc79P+K929wcxGA3jOzLa6+1teJKb+KawAgNJyvmdZCJE5zuvK7+4Nqc/NAB4FsOgMP7PS3Wvdvbao+OybCwohLiznbH4zKzaz0tNfA7gBwKbBWpgQ4sJyPk/7qwA8amanf89/u/vPBmVVQogLzjmb3913Abj0ncTk9Oah9FC47v5oaXgENwCUHQ3vEViV5HnZBWP4mOyGul9SfcLCcE39ljfeoLEfncHf6zh8dbjuHAAeOZCk+tyGaUHt7ht20tj1zXRrBvLTjJqe1cb3AVzbvDCo9T5xHY19btGPqT5289VUn7cjPIuhdf41NLZzP388jK7eR/VZ919F9Q3Xh62zu5E/gR42IrzfpT+PP1YGolSfEJEi8wsRKTK/EJEi8wsRKTK/EJEi8wsRKRlt3Z1rjpK83qBe3s7bJTdVlAW1ObW8ZXH5Tw5R/cRlfPfhr5vDJZpTFvGy1j119VQf18zbis+bwctD1ybDJcFXt/GUVeXwH1C9+Me8Nfeaz8+netvz4bbkC/t4Sqt1arjNOwDMOcZTqMMqwi3TNx99kcZu7uR/kxnJT1P90OSvUP3yZ8Kpwr6LeQl4bnu4HTr6cmnsQHTlFyJSZH4hIkXmFyJSZH4hIkXmFyJSZH4hIkXmFyJSMprn7+3LwaHW8Fjm3LGv0vg80p674ZWlNNYmH6T60fyjVB+XHy67rd7G89ENuy+meu8M3gYhueZaql9lHtRak7yx8mNlfG/FZbfwnPPCjXx/xQScCGoN5bzcuOnxcKkyAOzt5+2zC3rDo9M7vZHGLq2cTPUdjU9TfcyxS6i+cVT4MdOVx/8m7WS/S2+e8vxCiDTI/EJEiswvRKTI/EJEiswvRKTI/EJEiswvRKRkNM+PYf3wueHa9O07r6Dhi0eEc7O5v9NEYysffonqPYs+RvWmF18Jaq9P53sIJl7Dc+F7jn2S6h9c8xDVX/1G+H/41O28Ln14yUSqj3ihlepFebx7+768dUHt8v330tjVG/n9rvy7B6he19AW1PJL+GPt5GN8f8Qf3sLv93dK/pLqF9V+NqideCncmhsAKsaF15YH5fmFEGmQ+YWIFJlfiEiR+YWIFJlfiEiR+YWIFJlfiEhJm+c3swcA3ASg2d0vSd1WAeBHAKoB7AFwq7uH52enyO/NRfWhcK91M54vt65wH/aWe3hve5t1A9Vf+fwBql+0MHyqdheRPuoAhrXymQD5hzZS/eW/qqb6pLrwWOZHOnjf/b/ZdDnVv3bHl6k+87v8vn+k6sPh3z3/2zR2Xhnvy79nej/VL+ufG9SKfsVz6S/dcpzqK4fzev6c7i9RffPa8CyHGcv43orEzvBjPT/ZQ2MHcjZX/u8CWPa22z4H4AV3nw7ghdT3Qoh3EWnN7+6rALS87eabATyY+vpBALcM8rqEEBeYc33NX+X+mz5IhwBUDdJ6hBAZ4rzf8HN3BxBsImdmK8xsrZmtPdnRfr6HE0IMEudq/iYzGwsAqc/NoR9095XuXuvutSVFxed4OCHEYHOu5n8cwJ2pr+8E8JPBWY4QIlOkNb+Z/QDArwHMNLMDZvZxAPcCuN7MdgBYmvpeCPEuIm2e391vD0jXvfPD9cARzqc3juS15/3d4V7oU+fz3vk9uVupfuNh3qf9zaaGoDatjM+oHz+B92G/poDf7xeaeN36Ix/8g6C2sKuOxr76mV9TfdKTfB9A12J+/dhRdU1QW3LP39DY+j/eT/Xn1n2a6sNrnglqeSteprH7XuHbVhaMG0v1Yf51qk8d9rWg1rv792jsvpq/CmrJgv9LYweiHX5CRIrML0SkyPxCRIrML0SkyPxCRIrML0SkZHZEd9LQ2Bz+f9PfOYXGl1WHS36rjlXSWC/juwu338TLaoe1vT+ojeGVyOhueJLqv3jxeqrP+sQCqpe8MimoVe67isYe/uzPqd7x/8KpOgAYvZ+vvWLBnUGtcyENxRwbTvXKQp7W6mgOt0yf/iJvpz7sIB8P/nrlcqpfO/zHVH+sJ1wIO7ubnxjfcDisdYbLu9+OrvxCRIrML0SkyPxCRIrML0SkyPxCRIrML0SkyPxCREpm8/y5OTg0oiiotxzlLa4nV4XLT/f63TS29fV/pHp+H2/tPWpiuKy2rnQCjc0dzttnd02ppfrFjz1G9YYF4fHkTSX8nPZW/jHVlxTzPi3rPsrLlR/dWh3UbpkSHt8NAKv33kz15KLwyHYAKGsIl2Evv/MmGtvyHb7/oWEYLwl+ufNiqi8aHh6lnb+e5+oPzNkX1PoT3TR2ILryCxEpMr8QkSLzCxEpMr8QkSLzCxEpMr8QkSLzCxEpGc3zF/blYnpLWVAfVtNJ4xsOzghqeXtO0NgJk6+k+i96w+sCgMrq9wa19pYCGvvhRt7+uiiPj7n+58vC9foAUNK2JagtrOG9AJ4Ip4wBAO1Xz6P6/O/zfPesD4Tr+e+fy1tvz/sPvjejvnw91ce2hv9md/1oEY29Y+o2qpc/zEd4/8GS8Dh5APCCS4PafRUbaOyERjL2rpePLR+IrvxCRIrML0SkyPxCRIrML0SkyPxCRIrML0SkyPxCREraPL+ZPQDgJgDN7n5J6rZ7ANwN4HQD8S+4+1Ppfldvbi8OlYd7jvf3zaTxozxcn90/KtwnAAAKO/jI5WWv8try+gnhvG7VxH+gsYk9/0n14tF8j0FNa/icAcCVf/jXQe2ZrZ+lsV/6fjgXDgCrlr5K9b+/5qtU/7eOV4LabVt4nr9+Kd+bMfXAfVQvL/2zoLZ88iM09m+th+q7bv2fVF/e8Auqn6j8RlC7ZuZoGrtt32Vh0fio+oGczZX/uwCWneH2r7r7vNRHWuMLIYYWac3v7qsAtGRgLUKIDHI+r/k/ZWZ1ZvaAmZUP2oqEEBnhXM3/TQDTAMwD0AjgK6EfNLMVZrbWzNZ2dvC9+0KIzHFO5nf3Jnfvc/d+AN8CEKyScPeV7l7r7rXDingzSSFE5jgn85vZ2AHffgjApsFZjhAiU5xNqu8HAK4FMNLMDgD4awDXmtk8AA5gD4BPXMA1CiEuAGnN7+63n+Hm+8/lYAkHxvf2BfXOinBffgDYv3NKUDPfT2Pbr+R3defB2VSf9ibph17kNHZXJ+9tf8xWUz3ZfhXVy+/bHtQWTiU5YQBP9rxO9d6WcA8FAFg6jj957Ho03J++7sNVNLatbhrVF484QvX6R+cGte9/hPcCWNrN9xiUd/GZAVv28j4Hrx0vDmqTRuXT2ETV8KBmifD5fjva4SdEpMj8QkSKzC9EpMj8QkSKzC9EpMj8QkRKRlt3dyeS2DUmXFp7dDUfm1w6Kzw2eUI5LweuODSL6u+byNNxjfWHgtrOP6qmsWM28xHeZbt5SmvW/q1U37hkR1DLKVxCY7v/haes5m8JpxEBYEz7v1P95HXh8eILf/pFGtt0YhXV8dk3qHysY2RQ29N/NY39H528jLpm/k+pvqqMp57v2PFHQe2byaM0dv7Bk0EtR627hRDpkPmFiBSZX4hIkfmFiBSZX4hIkfmFiBSZX4hIyWieP78/H+M6Jgb14VfwfHb36nAZZEteL40dPpKP0T78S95lKO/6cAnooYf4CO7uXJ7nP7LPqG5VHVTfsincrnneiHDrbAC4bddfUP2RcTyfXfziAar/aOG/BbU/KwmPFgeA+67l+e5hb1xM9ev2h/dm1C4cT2N/vJWX/M4BbwXf2n4d1auKw+XMU7t4G/m+sovCYi5/LA1EV34hIkXmFyJSZH4hIkXmFyJSZH4hIkXmFyJSZH4hIiWjeX70AnmHwvXGU2vCY7ABoH5kuC1xTYKP6K7o3E31Q8mfUL2mKNwPYPScJhq7aA+vO3/iT+6i+swXn6X6Tw+H+wGMv5Lvb1h/z5NUP/Ylrk8Y/k9Uv+tX/xzUlt86n8Y+zztzY/O0OVQv2FYS1F58hI9V//0/uY3qL3TyUdj7y8ZRfXHhm0HtyDDSJh7AsWR4r0xPDm/7PRBd+YWIFJlfiEiR+YWIFJlfiEiR+YWIFJlfiEiR+YWIlLR5fjObCOB7AKoAOICV7v51M6sA8CMA1QD2ALjV3WmRc04/UNAZrjfesI/3HG/PSYR/d4L3nz+wO1zbDQB7J7+P6h3HwnsQ1pfvobHTOjup/p7GcA93AFg7iY8PX9Ye7nNQuYvvb0j8b16v/5H7P0P1/XfUU31bbzhZv+/pSho7qvoKqk9YzWvXSyu2BbUp87to7DM/5PsbGir52q7L5b0KXloU1q5IM25+Y274b2rgewQGcjZX/iSAP3X32QAuB/BJM5sN4HMAXnD36QBeSH0vhHiXkNb87t7o7utTX7cB2AJgPICbATyY+rEHAdxyoRYphBh83tFrfjOrBjAfwGoAVe5++rn2IZx6WSCEeJdw1uY3sxIADwP4jLu3DtTc3XHq/YAzxa0ws7Vmtra9i/eiE0JkjrMyv5klcMr4/+Xuj6RubjKzsSl9LIDmM8W6+0p3r3X32uJCXnwjhMgcac1vZgbgfgBb3P1fB0iPA7gz9fWdAHhZnBBiSHE2Jb2LAXwMQL2ZbUjd9gUA9wJ4yMw+DmAvgFvT/aJkXj8OV4THC1fm8dTNjER7UKufStoZAxjV/RrVJ88aTvWKugVB7dJC3gb64bk87bOlIVyiCQDfWvV+qv/9p9YEtdL9PPVz47S/pXpBFV/7pl/wEeBTfyd83n++ip/z/O/yFOmuj56gen/T2KB2ywze6r03yUe+Pz5rGdVzyr5N9aqXw4+Zzsk8jTg5GU6vFvTwczaQtOZ395cBhFzJm5MLIYYs2uEnRKTI/EJEiswvRKTI/EJEiswvRKTI/EJESkZbd5vlozBRHdQbusN5fAAoTITLQyt28btSUvUBqo/dwsc9PzFtdVBbUNxDYy/a/Amqv/ckHw/+jXm/pvrsn48Ka7N5C+m2b1xD9W9+fh/Vl30t3IIaADZ+dWlQq1nC7/fJ+Xy8+KRd4T0jAHDDqElB7Zf/ze/3tusepHpi+1NUb35vuM08ACQqwuXMm7r437t8Zng/TJJ3an8LuvILESkyvxCRIvMLESkyvxCRIvMLESkyvxCRIvMLESkZzfN7bi+6h4dbaI86wGuROyaFa7B7Ww7S2NadB6heXMzjP7ivIag9PJOPc5408h+pbuU8X11bfMYOab/h+a4vBrXJI/+Oxpb2hsd7A0DlU7+k+tOTbqf6n08M5+pP1O2isWuu4GOwZyb+nOp7X7k7qFXfzNtjt/XfQfWm2iTVy7t5vwAknghKPWN4O8yDfTPCseDnbCC68gsRKTK/EJEi8wsRKTK/EJEi8wsRKTK/EJEi8wsRKRnN8yMJ5LSEx3DvLud9+2e3lAU172uhsSNL+6i+OcH/D+Y2hfu4jzrC8/QLDvMe8C2jDlN95wHeG398SV1QG/6z8LwBANgx6jtUX1bG8/iHn+T7I/bPD9f7d/6f52ls/2PXU707wWvuSysuDWp7fhbugQAALe/bSPXxb06m+snNPM9v7wk/ljva+TyDeb3hWQzPJvmY+4Hoyi9EpMj8QkSKzC9EpMj8QkSKzC9EpMj8QkSKzC9EpKTN85vZRADfA1AFwAGsdPevm9k9AO4GcDpJ/QV3p83M3YAekk+fAT5L/qS1htd5Ea95b2kL53wBoLx5DNXn1m0NapsXvkxjnx7Pc+G1W/g+gCkvraJ61wfDvfmT0/h5Kd7Fz0tXT3heAQD0XVVB9eOLwz3o9z70uzT2WOMGqi8czZvUHys7EdRG3VZOY2te5/X6z04ZSfUZH9hJ9d1Ph8/73BLee6J/RFj3/jR9BAZwNpt8kgD+1N3Xm1kpgHVm9lxK+6q7/8tZH00IMWRIa353bwTQmPq6zcy2ABh/oRcmhLiwvKPX/GZWDWA+gNPPBT9lZnVm9oCZnfF5lJmtMLO1Zra2o6PjvBYrhBg8ztr8ZlYC4GEAn3H3VgDfBDANwDycembwlTPFuftKd69199qioqJBWLIQYjA4K/ObWQKnjP9f7v4IALh7k7v3uXs/gG8BWHThlimEGGzSmt/MDMD9ALa4+78OuH3sgB/7EIBNg788IcSF4mze7V8M4GMA6s3sdO7lCwBuN7N5OJX+2wOAz6EGYJ5AXnJ0UO9PTqHxY4+GU2Zr8nk58OWd/P2GvS35VF8/JjxOOv/QRTR2ai5/f7RlOk87dbc2UX3vgfC45/xq/lKrbjS/3/N28DbS7Xl7qd55eEVQm1PE/yarpk6n+uYS3uq9j4w+n1bNS8DfrOclv5c3hMfFA0DxCR5/cSKcptw2mqdni1pKg5r3nf3beGfzbv/LAM7kLD6gXAgxpNEOPyEiReYXIlJkfiEiReYXIlJkfiEiReYXIlIy2rrb8npQWBEedZ1MTKTxDaXhvPDI7kk0treDl1jmO2+/3VRJjj08nGcHgNJO3pq7dRNvt3ykMJzXBYAp89uC2u6t+2hs6XjegnpnVyPVR1TxteXt3B3UjibStGo/uZnqZXkLqV5YEB7D/fxrNTR28qzw4xQAutrCZdQAkD+Ml6cXj1gX1Ky4hMaOnBQew533s7O/nuvKL0SkyPxCRIrML0SkyPxCRIrML0SkyPxCRIrML0SkmDuvHR7Ug5kdBjCwAHwkAF4YnT2G6tqG6roAre1cGcy1TXZ33kwgRUbN/1sHN1vr7rVZWwBhqK5tqK4L0NrOlWytTU/7hYgUmV+ISMm2+Vdm+fiMobq2obouQGs7V7Kytqy+5hdCZI9sX/mFEFkiK+Y3s2Vmts3MdprZ57KxhhBmtsfM6s1sg5mtzfJaHjCzZjPbNOC2CjN7zsx2pD7zvt+ZXds9ZtaQOncbzGx5ltY20cxeNLPNZvaGmf2v1O1ZPXdkXVk5bxl/2m9muQC2A7gewAEArwG43d158XaGMLM9AGrdPes5YTO7GsBJAN9z90tSt/0TgBZ3vzf1j7Pc3f9iiKztHgAnsz25OTVQZuzAydIAbgFwF7J47si6bkUWzls2rvyLAOx0913u3gPghwBuzsI6hjzuvgrA26dL3AzgwdTXD+LUgyfjBNY2JHD3Rndfn/q6DcDpydJZPXdkXVkhG+YfD2Bgi5UDGFojvx3As2a2zszC42ayR1VqbDoAHALAR+pknrSTmzPJ2yZLD5lzdy4TrwcbveH321zp7gsA3Ajgk6mnt0MSP/WabSila85qcnOmOMNk6d+QzXN3rhOvB5tsmL8BwMBmfRNStw0J3L0h9bkZwKMYetOHm04PSU19bs7yen7DUJrcfKbJ0hgC524oTbzOhvlfAzDdzKaYWT6A2wA8noV1/BZmVpx6IwZmVgzgBgy96cOPA7gz9fWdAH6SxbW8haEyuTk0WRpZPndDbuK1u2f8A8BynHrH/00Af5mNNQTWNRXAxtTHG9leG4Af4NTTwF6cem/k4wAqAbwAYAeA5wFUDKG1fR9APYA6nDLa2Cyt7UqcekpfB2BD6mN5ts8dWVdWzpt2+AkRKXrDT4hIkfmFiBSZX4hIkfmFiBSZX4hIkfmFiBSZX4hIkfmFiJT/DycpXdyk07M2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "z_dim = 100\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "epochs = 2\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
